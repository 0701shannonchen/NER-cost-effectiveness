{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import dotenv\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag: B-art  Precision: 0.0629, Recall: 0.1163, F1 Score: 0.0816, Support: 86\n",
      "Tag: B-eve  Precision: 0.0864, Recall: 0.1167, F1 Score: 0.0993, Support: 60\n",
      "Tag: B-geo  Precision: 0.7359, Recall: 0.7828, F1 Score: 0.7586, Support: 7664\n",
      "Tag: B-gpe  Precision: 0.6985, Recall: 0.8441, F1 Score: 0.7644, Support: 3175\n",
      "Tag: B-nat  Precision: 0.0318, Recall: 0.1000, F1 Score: 0.0483, Support: 50\n",
      "Tag: B-org  Precision: 0.5857, Recall: 0.5553, F1 Score: 0.5701, Support: 3913\n",
      "Tag: B-per  Precision: 0.6711, Recall: 0.6707, F1 Score: 0.6709, Support: 3389\n",
      "Tag: B-tim  Precision: 0.6101, Recall: 0.7819, F1 Score: 0.6854, Support: 4049\n",
      "Tag: I-art  Precision: 0.0347, Recall: 0.0862, F1 Score: 0.0495, Support: 58\n",
      "Tag: I-eve  Precision: 0.1143, Recall: 0.1509, F1 Score: 0.1301, Support: 53\n",
      "Tag: I-geo  Precision: 0.4789, Recall: 0.6903, F1 Score: 0.5655, Support: 1450\n",
      "Tag: I-gpe  Precision: 0.0540, Recall: 0.5385, F1 Score: 0.0981, Support: 39\n",
      "Tag: I-nat  Precision: 0.0390, Recall: 0.2500, F1 Score: 0.0674, Support: 12\n",
      "Tag: I-org  Precision: 0.6336, Recall: 0.5249, F1 Score: 0.5742, Support: 3315\n",
      "Tag: I-per  Precision: 0.8192, Recall: 0.7483, F1 Score: 0.7822, Support: 3445\n",
      "Tag: I-tim  Precision: 0.3243, Recall: 0.4762, F1 Score: 0.3858, Support: 1300\n",
      "Tag: O  Precision: 0.9838, Recall: 0.9660, F1 Score: 0.9748, Support: 176877\n",
      "\n",
      "Overall Precision (excluding 'O'): 0.6537, Overall Recall (excluding 'O'): 0.6952, Overall F1 Score (excluding 'O'): 0.6697\n"
     ]
    }
   ],
   "source": [
    "processed_df = pd.read_csv('data/test_predictions_gpt3.5_zero.csv')\n",
    "\n",
    "# Convert the 'Tag' and 'Predicted_Tags' columns from strings to lists\n",
    "processed_df['Tag'] = processed_df['Tag'].apply(ast.literal_eval)\n",
    "processed_df['Predicted_Tags'] = processed_df['Predicted_Tags'].apply(ast.literal_eval)\n",
    "\n",
    "# Flatten the 'Tag' and 'Predicted_Tags' columns into lists\n",
    "actual_tags = [tag for row in processed_df['Tag'] for tag in row]\n",
    "predicted_tags = [tag for row in processed_df['Predicted_Tags'] for tag in row]\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each tag\n",
    "precision, recall, f1, support = precision_recall_fscore_support(actual_tags, predicted_tags, average=None, labels=np.unique(actual_tags))\n",
    "\n",
    "# Map these metrics to each unique tag\n",
    "tag_metrics = dict()\n",
    "unique_tags = np.unique(actual_tags)\n",
    "for i, tag in enumerate(unique_tags):\n",
    "    tag_metrics[tag] = {\n",
    "        'precision': precision[i],\n",
    "        'recall': recall[i],\n",
    "        'f1_score': f1[i],\n",
    "        'support': support[i]\n",
    "    }\n",
    "\n",
    "# Display the metrics for each tag\n",
    "for tag, metrics in tag_metrics.items():\n",
    "        print(f\"Tag: {tag}  Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, F1 Score: {metrics['f1_score']:.4f}, Support: {metrics['support']}\")\n",
    "        \n",
    "# Get unique tags excluding 'O'\n",
    "unique_tags = np.unique([tag for tag in actual_tags if tag != 'O'])\n",
    "        \n",
    "# Calculate overall metrics excluding 'O' tag\n",
    "overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(actual_tags, predicted_tags, average='weighted', labels=unique_tags)\n",
    "\n",
    "\n",
    "# Display overall metrics excluding 'O' tag\n",
    "print(f\"\\nOverall Precision (excluding 'O'): {overall_precision:.4f}, Overall Recall (excluding 'O'): {overall_recall:.4f}, Overall F1 Score (excluding 'O'): {overall_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
